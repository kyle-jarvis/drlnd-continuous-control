# Udacity DRLND
## P2 Continous Control

### Environment

Agents control an actuated limb by applying torque to its joints. Agents
receive a reward when the grabber, attached to the end of the limb, is in a
designated target zone. The target zone is in motion, and its trajectory is
randomised for each episode.

The environment is considered to have been resolved when the agent collects an average reward of at least +30 over 100 episodes.

The agent observes a 33-dimensional vector representation of the state, where
the components of the vector encode the current orientation and motion of both
the actuated limb and the target zone.

The action space is 4-dimensional and continuous.

## Learning Algorithm

The learning algorithm used to collect the below results is the Deep Deterministic
Policy Gradients algorithm described [here](https://arxiv.org/pdf/1509.02971.pdf).
Agents use a pair of networks to approximate the action-value function and to
encode a policy. Experiences are stored in a replay buffer, and mini-batches are sampled uniformly from this experience buffer at each update step.



For the results illustrated below, the architecture was as follows:

A. Actor
1. An input layer of (33) neurons.
2. A first hidden layer of 256 neurons.
3. A second hidden layer of 256 neurons.
4. An output layer of 4 neurons.

B. Critic
1. An input layer of (33) + (4) neurons.
2. A first hidden layer of 256 neurons.
3. A second hidden layer of 256 neurons.
4. An output layer of 1 neurons.

All layers are fully-connected, with ReLU activations applied to the neurons, except
for the output layers, where the Actor network has a tanh activation applied and
the critic has a simple linear activation.

Training proceeds in the usual way. During training, noise can be added to the agents
action selection. This can lead to more or less efficient exploration of the environment. 
In particular, as highlighted in [here](https://arxiv.org/pdf/1509.02971.pdf), 
a particularly useful form of noise for use in physical/mechanical simulations
with intertia is auto-correlated noise that can be obtained from a 
Uhlenbeck/Orstein process. For this report we have investigated how the noise 
distribution impacts the learning rate. 

Most layers in the actor/critic networks have their weights and biases intialised according to the Glorot/Xavier-Uniform [method](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_), however, the output / final layers of the two networks are initialised with their weights and biases close to zero, as in the Deepmind DDPG paper.

### Hyperparameters

1. Learning rate : 5e-4

2. Tau (soft update parameter) : 1e-3

3. Minibatch size : 64

4. Memory buffer size : 5e5

5. Gamma : 0.99

6. Noise distribution (normal):
    - σ : 1.0 → 0.0 over 250 episodes.
    - μ : 0.0

7. Noise distribution ([Ornstein Uhlenbeck](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)):
    - θ : 0.15
    - σ : 1.0 → 0.0 over 250 episodes.
    - μ : 0.0
    - dt : 0.01

## Results

We have investigated how the learning process is affected by the particular
distribution of noise that is added to the agent's actions in order to define
the exploration policy. We have recorded how the agent learns when there is:

1. A noise generating process where noise is sampled from a normal distribution. The noise at each time step within one episode is identical and independent. The width of the normal distribution is linearly tapered to 0 over 250 episodes.
2. A noise generating process where noise is sampled from an Ornstein Uhlenbeck process. The noise generated by this process if autocorrelated, which helps in simulations of physical systems where inertial is present. The region explored by the noise generating process is tapered to 0 over 250 episodes.

Interestingly, despite the OU process being touted as an efficient exploration policy in these types of environments, we find the agent learns quicker when the noise generating process is uncorrelated.

This is probably not a limitation in this environment, because the target area is in motion, very often intersecting with the arm to generate a reward, and so even with an inefficient exploration policy (in terms of physical space explored) there is strong discrimination between states that can be learned from.

<img src = "./resources/comparison.png" width="300"/>

The GIF below shows a set of articulated limbs being controlled by the agent trained with the Normal noise generating process in the image above.

<img src = "./resources/reacher.gif" width="300" height=180/>

## Future work

The code included in this repository can be extended in order to investigate
various aspects of reinforcement learning in this simple setting. 

1) Given that the noise distribution employed in the exploration policy bears a significant impact on the learning curves, the exploration of the parameters that govern this noise could be a source of further gains in the learning rate and ultimate performance.
2) A dynamic mini-batch size, which increases over time, might be beneficial to fine-tuning the agent in the later part of the training, where the agent receives the same reward most of the time. At this stage most transitions will yield the same reward, and so the likelihood of there being useful information in a given minibatch of transitions decreases.